{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = true\n",
       "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = true\n",
    "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\",2:\"<unk>\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 20052 sentence pairs\n",
      "Trimmed to 1603 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "chn 9\n",
      "eng 1378\n",
      "[' ', 'you re going to be a mommy .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'chn', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lang.word2count['i']\n",
    "eng = output_lang\n",
    "eng_words = output_lang.index2word.values()\n",
    "#print(list(eng_words))\n",
    "big_sentence_array = [p[1].split() for p in pairs]\n",
    "big_sentence_array = [p.split('.') for arr in big_sentence_array for p in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = reduce(lambda x,y:x+y,big_sentence_array)\n",
    "test_sentence = [w for w in test_sentence if w!='']\n",
    "for index in range(len(test_sentence)):\n",
    "    if test_sentence[index] == 'm':\n",
    "        test_sentence[index]='am'\n",
    "    elif test_sentence[index] == 're':\n",
    "        test_sentence[index]='are'\n",
    "    elif test_sentence[index] == 's':\n",
    "        test_sentence[index]='is'\n",
    "#vocab = set(test_sentence)\n",
    "#word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['i', 'am'], 'ok'), (['am', 'ok'], 'i'), (['ok', 'i'], 'am')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "word_to_ix['<unk>'] = len(word_to_ix)\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "model = NGramLanguageModeler(len(word_to_ix), EMBEDDING_DIM,CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 62547.0312])\n",
      "tensor([ 52094.1094])\n",
      "tensor([ 49216.1797])\n",
      "tensor([ 47509.8047])\n",
      "tensor([ 46312.9883])\n",
      "tensor([ 45372.3516])\n",
      "tensor([ 44578.9375])\n",
      "tensor([ 43877.5469])\n",
      "tensor([ 43238.2891])\n",
      "tensor([ 42643.6367])\n",
      "tensor([ 42083.3125])\n",
      "tensor([ 41550.6211])\n",
      "tensor([ 41039.3984])\n",
      "tensor([ 40545.8242])\n",
      "tensor([ 40065.9062])\n",
      "tensor([ 39598.1602])\n",
      "tensor([ 39140.7891])\n",
      "tensor([ 38692.7344])\n",
      "tensor([ 38252.2812])\n",
      "tensor([ 37818.5508])\n",
      "tensor([ 37391.3984])\n",
      "tensor([ 36970.4023])\n",
      "tensor([ 36555.2891])\n",
      "tensor([ 36145.2578])\n",
      "tensor([ 35740.6602])\n",
      "tensor([ 35341.5078])\n",
      "tensor([ 34947.3125])\n",
      "tensor([ 34558.5547])\n",
      "tensor([ 34175.3711])\n",
      "tensor([ 33797.9531])\n",
      "tensor([ 33426.3789])\n",
      "tensor([ 33060.6875])\n",
      "tensor([ 32701.2422])\n",
      "tensor([ 32347.4434])\n",
      "tensor([ 32000.4238])\n",
      "tensor([ 31659.5195])\n",
      "tensor([ 31325.6465])\n",
      "tensor([ 30998.1484])\n",
      "tensor([ 30677.5781])\n",
      "tensor([ 30364.2441])\n",
      "tensor([ 30057.5078])\n",
      "tensor([ 29758.0195])\n",
      "tensor([ 29465.3691])\n",
      "tensor([ 29179.4844])\n",
      "tensor([ 28900.6348])\n",
      "tensor([ 28628.7910])\n",
      "tensor([ 28363.3145])\n",
      "tensor([ 28104.8574])\n",
      "tensor([ 27852.5234])\n",
      "tensor([ 27606.7012])\n",
      "[tensor([ 62547.0312]), tensor([ 52094.1094]), tensor([ 49216.1797]), tensor([ 47509.8047]), tensor([ 46312.9883]), tensor([ 45372.3516]), tensor([ 44578.9375]), tensor([ 43877.5469]), tensor([ 43238.2891]), tensor([ 42643.6367]), tensor([ 42083.3125]), tensor([ 41550.6211]), tensor([ 41039.3984]), tensor([ 40545.8242]), tensor([ 40065.9062]), tensor([ 39598.1602]), tensor([ 39140.7891]), tensor([ 38692.7344]), tensor([ 38252.2812]), tensor([ 37818.5508]), tensor([ 37391.3984]), tensor([ 36970.4023]), tensor([ 36555.2891]), tensor([ 36145.2578]), tensor([ 35740.6602]), tensor([ 35341.5078]), tensor([ 34947.3125]), tensor([ 34558.5547]), tensor([ 34175.3711]), tensor([ 33797.9531]), tensor([ 33426.3789]), tensor([ 33060.6875]), tensor([ 32701.2422]), tensor([ 32347.4434]), tensor([ 32000.4238]), tensor([ 31659.5195]), tensor([ 31325.6465]), tensor([ 30998.1484]), tensor([ 30677.5781]), tensor([ 30364.2441]), tensor([ 30057.5078]), tensor([ 29758.0195]), tensor([ 29465.3691]), tensor([ 29179.4844]), tensor([ 28900.6348]), tensor([ 28628.7910]), tensor([ 28363.3145]), tensor([ 28104.8574]), tensor([ 27852.5234]), tensor([ 27606.7012])]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(1)\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    print(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅保存和加载模型参数(推荐使用)\n",
    "torch.save(model.state_dict(), 'lm-model.pkl')\n",
    "#model.load_state_dict(torch.load('lm-model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-3060cf88f854>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-3060cf88f854>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
